#

## Общие параметры

В этом разделе мы рассмотрим наиболее часто используемые опции для запуска `основной` программы с моделями LLaMA:

- `-m FNAME, --model FNAME`: Укажите путь к файлу модели LLaMA (например, `models/7B/ggml-model.bin`).
- `-i, --interactive`: Запустите программу в интерактивном режиме, что позволит вам вводить данные напрямую и получать ответы в режиме реального времени.
- `-ins, --instruct`: Запустите программу в режиме инструкций, что особенно полезно при работе с моделями альпака.
- `-n N, --n-predict N`: Установите количество токенов для прогнозирования при генерации текста. Настройка этого значения может повлиять на длину сгенерированного текста.
- `-c N, --ctx-size N`: Установите размер контекста приглашения. Значение по умолчанию равно 512, но модели LLaMA были построены с контекстом 2048, что обеспечит лучшие результаты при более длительном вводе/логическом выводе.

## Подсказки ввода

Программа "main" предоставляет несколько способов взаимодействия с моделями LLaMA с помощью подсказок ввода:

- `--prompt ПРИГЛАШЕНИЕ`: Предоставьте приглашение непосредственно в качестве параметра командной строки.
- `--file FNAME`: Укажите файл, содержащий приглашение или несколько приглашений.
- `--interactive-first`: Запустите программу в интерактивном режиме и сразу дождитесь ввода. (Подробнее об этом ниже.)
- `--random-prompt`: Начните с рандомизированного приглашения.

## Взаимодействие

Программа "main" предлагает простой способ взаимодействия с моделями LLaMA, позволяя пользователям вести беседы в режиме реального времени или предоставлять инструкции для выполнения конкретных задач. Интерактивный режим может быть запущен с использованием различных опций, включая "--interactive", "--interactive-first` и `--instruct`.

В интерактивном режиме пользователи могут участвовать в генерации текста, вводя свои данные во время процесса. Пользователи могут в любое время нажать "Ctrl+C", чтобы вставить и ввести свои данные, а затем нажать "Return", чтобы отправить их в модель LLaMA. Чтобы отправить дополнительные строки без завершения ввода, пользователи могут завершить текущую строку обратной косой чертой (`\`) и продолжить ввод.

### Параметры взаимодействия

- `-i, --interactive`: Запуск программы в интерактивном режиме, позволяющий пользователям участвовать в беседах в режиме реального времени или давать конкретные инструкции модели.
- `--interactive-first`: Запустите программу в интерактивном режиме и немедленно дождитесь ввода пользователем, прежде чем начинать генерацию текста.
- `-ins, --instruct`: Запустите программу в режиме инструкции, который специально разработан для работы с моделями Alpaca, которые превосходно справляются с задачами, основанными на инструкциях пользователя.
- `--color`: Включите цветной вывод, чтобы визуально различать подсказки, вводимые пользователем, и сгенерированный текст.

Понимая и используя эти варианты взаимодействия, вы можете создавать увлекательные и динамичные взаимодействия с моделями LLaMA, адаптируя процесс создания текста к вашим конкретным потребностям.

### Обратные подсказки

Обратные приглашения - это мощный способ создать впечатление чата с моделью LLaMA, приостанавливая генерацию текста при обнаружении определенных текстовых строк:

- `-r ПРИГЛАШЕНИЕ, --reverse-prompt ПРИГЛАШЕНИЕ`: Укажите одно или несколько обратных приглашений, чтобы приостановить генерацию текста и переключиться в интерактивный режим. Например, `-r "Пользователь:"` можно использовать для возврата к разговору всякий раз, когда наступает очередь пользователя говорить. Это помогает создать более интерактивный и разговорный опыт. Однако обратное приглашение не работает, если оно заканчивается пробелом.

Чтобы преодолеть это ограничение, вы можете использовать флаг "--in-prefix", чтобы добавить пробел или любые другие символы после обратного приглашения.

### In-Prefix

Флаг `--in-prefix` используется для добавления префикса к вашим вводимым данным, в первую очередь, он используется для вставки пробела после обратного приглашения. Вот пример того, как использовать флаг `--in-prefix` в сочетании с флагом `--reverse-prompt':

``sh
./main -r "Пользователь:" --in-prefix " "
```

### В-суффикс

Флаг `--in-suffix` используется для добавления суффикса после вашего ввода. Это полезно для добавления запроса "Помощник:" после ввода пользователем. Он добавляется после символа новой строки (`\n`), который автоматически добавляется в конец пользовательского ввода. Вот пример того, как использовать флаг `--in-suffix` в сочетании с флагом `--reverse-prompt':

``sh
./main -r "Пользователь:" --в-префиксе " " --в-суффиксе "Ассистент:"
```

### Режим инструкций

Режим инструкций особенно полезен при работе с моделями Alpaca, которые предназначены для выполнения инструкций пользователя при выполнении конкретных задач:

- `-ins, --instruct`: Включите режим инструкций, чтобы использовать возможности моделей Alpaca при выполнении задач на основе предоставленных пользователем инструкций.

Техническая деталь: вводимые пользователем данные имеют внутренний префикс обратного приглашения (или "### Инструкция:" по умолчанию), за которым следует "### Ответ:" (за исключением случаев, когда вы просто нажимаете Return без каких-либо вводимых данных, чтобы продолжать генерировать более длинный ответ).

Понимая и используя эти варианты взаимодействия, вы можете создавать увлекательные и динамичные взаимодействия с моделями LLaMA, адаптируя процесс создания текста к вашим конкретным потребностям.

## Управление контекстом

Во время генерации текста модели LLaMA имеют ограниченный размер контекста, что означает, что они могут учитывать только определенное количество токенов из входного и сгенерированного текста. Когда контекст заполняется, модель внутренне перезагружается, потенциально теряя некоторую информацию из начала разговора или инструкций. Параметры управления контекстом помогают поддерживать непрерывность и согласованность в таких ситуациях.

### Размер контекста

Параметр `--ctx-size` позволяет задать размер контекста запроса, используемого моделями LLaMA при генерации текста. Больший размер контекста помогает модели лучше понимать и генерировать ответы для более длительного ввода или разговоров.

- `-c N, --ctx-size N`: Установите размер контекста запроса (по умолчанию: 512). Модели LLaMA были построены с контекстом 2048, который даст наилучшие результаты при более длительном вводе/логическом выводе. Однако увеличение размера контекста сверх 2048 может привести к непредсказуемым результатам.

### Увеличенный размер контекста

Некоторые точно настроенные модели увеличили длину контекста за счет масштабирования RoPE. Например, если исходная предварительно обученная модель имеет длину контекста (максимальную длину последовательности) 4096 (4k), а точно настроенная модель имеет 32k. Это коэффициент масштабирования, равный 8, и он должен работать, если установить указанное выше значение "--ctx-size" равным 32768 (32k), а `--rope-scale` равным 8.

- `--rope-scale N`: где N - коэффициент линейного масштабирования, используемый в точно настроенной модели.

### Сохраняйте подсказку

Параметр `--keep` позволяет пользователям сохранять исходное приглашение, когда модель выходит из контекста, обеспечивая сохранение связи с исходной инструкцией или темой разговора.

- `--keep N`: Укажите количество токенов из исходного приглашения, которые необходимо сохранить, когда модель сбросит свой внутренний контекст. По умолчанию это значение равно 0 (что означает, что токены не сохраняются). Используйте `-1`, чтобы сохранить все токены из первоначального приглашения.

Используя параметры управления контекстом, такие как `--ctx-size` и `--keep`, вы можете поддерживать более согласованное взаимодействие с моделями LLaMA, гарантируя, что сгенерированный текст остается соответствующим исходному приглашению или диалогу.

## Флаги генерации

Следующие опции позволяют вам управлять процессом генерации текста и точно настраивать разнообразие, креативность и качество генерируемого текста в соответствии с вашими потребностями. Регулируя эти опции и экспериментируя с различными комбинациями значений, вы можете найти наилучшие настройки для вашего конкретного случая использования.

### Количество токенов для прогнозирования

- `-n N, --n-predict N`: Установите количество токенов для прогнозирования при генерации текста (по умолчанию: 128, -1 = бесконечность, -2 = до заполнения контекста)

Параметр `--n-predict` управляет количеством токенов, генерируемых моделью в ответ на запрос ввода. Регулируя это значение, вы можете влиять на длину генерируемого текста. Более высокое значение приведет к более длинному тексту, в то время как меньшее значение приведет к более короткому тексту.

Значение -1 позволит генерировать бесконечный текст, даже если у нас конечное контекстное окно. Когда контекстное окно заполнится, некоторые из более ранних токенов (половина токенов после `--n-keep`) будут отброшены. Затем контекст должен быть повторно оценен, прежде чем генерация сможет возобновиться. В больших моделях и/или больших контекстных окнах это приведет к значительной паузе в выводе.

Если пауза нежелательна, значение -2 немедленно остановит генерацию, когда контекст будет заполнен.

Важно отметить, что сгенерированный текст может быть короче указанного количества токенов, если встречается токен конца последовательности (EOS) или запрос обратного вызова. В интерактивном режиме генерация текста приостанавливается, и управление возвращается пользователю. В неинтерактивном режиме программа завершится. В обоих случаях генерация текста может прекратиться до достижения указанного значения `n-прогноз`. Если вы хотите, чтобы модель продолжала работать, даже не создавая конец последовательности самостоятельно, вы можете использовать параметр `--ignore-eos`.

### Температура

- `--temp N`: Отрегулируйте случайность генерируемого текста (по умолчанию: 0.8).

Температура - это гиперпараметр, который управляет случайностью генерируемого текста. Он влияет на распределение вероятностей выходных токенов модели. Более высокая температура (например, 1,5) делает выходные данные более случайными и креативными, в то время как более низкая температура (например, 0,5) делает выходные данные более целенаправленными, детерминированными и консервативными. Значение по умолчанию равно 0.8, что обеспечивает баланс между случайностью и детерминизмом. В крайнем случае, температура, равная 0, всегда будет выбирать наиболее вероятный следующий токен, что приведет к идентичным результатам при каждом запуске.

Пример использования: `--temp 0.5`

### Штраф за повтор

- `--repeat-penalty N`: Управляет повторением последовательностей токенов в сгенерированном тексте (по умолчанию: 1.1).
- `--repeat-last-n N`: Последние n токенов, которые необходимо учитывать для наказания за повторение (по умолчанию: 64, 0 = отключено, -1 = ctx-размер).
- `--no-penalize-nl`: Отключите пенализацию для токенов новой строки при применении штрафа за повторение.

Опция "repeat-penalize" помогает предотвратить генерирование моделью повторяющегося или монотонного текста. Более высокое значение (например, 1.5) будет более строго наказывать за повторения, в то время как более низкое значение (например, 0.9) будет более мягким. Значение по умолчанию - 1.1.

Параметр `repeat-last-n` управляет количеством токенов в истории, которые следует учитывать для наказания за повторение. Большее значение будет отображаться дальше в сгенерированном тексте, чтобы предотвратить повторения, в то время как меньшее значение будет учитывать только последние токены. Значение 0 отключает штраф, а значение -1 устанавливает количество токенов, считающихся равными размеру контекста (`ctx-size`).

Используйте опцию `--no-penalize-nl`, чтобы отключить штраф за перевод строки при применении штрафа за повтор. Эта опция особенно полезна для создания разговоров в чате, диалогов, кода, поэзии или любого текста, где символы перевода строки играют значительную роль в структуре и форматировании. Отключение штрафования за перевод строки помогает сохранить естественный поток и предполагаемое форматирование в этих конкретных случаях использования.

Пример использования: `--repeat-penalty 1.15 --repeat-last-n 128 --no-penalize-nl`

### Выборка Top-K

- `--top-k N`: Ограничьте выбор следующего токена K наиболее вероятными токенами (по умолчанию: 40).

Выборка Top-k - это метод генерации текста, который выбирает следующий токен только из k наиболее вероятных токенов, предсказанных моделью. Это помогает снизить риск генерации маловероятных или бессмысленных токенов, но также может ограничить разнообразие выходных данных. Более высокое значение для top-k (например, 100) будет учитывать больше токенов и приведет к более разнообразному тексту, в то время как более низкое значение (например, 10) сосредоточит внимание на наиболее вероятных токенах и сгенерирует более консервативный текст. Значение по умолчанию - 40.

Пример использования: `--top-k 30`

### Выборка Top-P

- `--top-p N`: Ограничьте выбор следующего токена подмножеством токенов с кумулятивной вероятностью выше порогового значения P (по умолчанию: 0,9).

Выборка Top-p, также известная как выборка nucleus, - это еще один метод генерации текста, который выбирает следующий токен из подмножества токенов, которые вместе имеют совокупную вероятность не менее p. Этот метод обеспечивает баланс между разнообразием и качеством, учитывая как вероятности токенов, так и количество токенов для выборки. Более высокое значение top-p (например, 0,95) приведет к более разнообразному тексту, в то время как более низкое значение (например, 0,5) приведет к созданию более сфокусированного и консервативного текста. Значение по умолчанию равно 0,9.

Пример использования: `--top-p 0.95`

### Минимальная выборка P

- `--min-p N`: Устанавливает минимальный порог базовой вероятности для выбора токена (по умолчанию: 0,05).

Метод выборки Min-P был разработан как альтернатива Top-P и направлен на обеспечение баланса качества и разнообразия. Параметр *p* представляет минимальную вероятность того, что токен будет рассмотрен, относительно вероятности наиболее вероятного токена. Например, при *p*=0,05 и наиболее вероятном токене, имеющем вероятность 0,9, логиты со значением меньше 0,045 отфильтровываются.

Пример использования: `--min-p 0,05`

### Выборка без хвоста (TFS)

- `--tfs N`: Включить выборку без хвоста с параметром z (по умолчанию: 1.0, 1.0 = отключено).

Выборка без хвостов (TFS) - это метод генерации текста, направленный на уменьшение влияния менее вероятных токенов, которые могут быть менее релевантными, менее связными или бессмысленными, на выходные данные. Аналогично Top-P, он пытается динамически определить основную часть наиболее вероятных токенов. Но TFS отфильтровывает логиты на основе второй производной их вероятностей. Добавление токенов прекращается после того, как сумма вторых производных достигает параметра z. Вкратце: TFS смотрит, как быстро уменьшаются вероятности токенов, и отсекает хвост маловероятных токенов, используя параметр z. Типичные значения для z находятся в диапазоне от 0,9 до 0,95. Значение 1,0 будет включать все токены и, таким образом, отключит эффект TFS.

Пример использования: `--tfs 0,95`

### Локально типичная выборка

- `--типичный N`: Включить локально типичную выборку с параметром p (по умолчанию: 1.0, 1.0 = отключено).

Локально типичная выборка способствует созданию контекстуально согласованного и разнообразного текста путем выборки лексем, которые являются типичными или ожидаемыми на основе окружающего контекста. Установив параметр p в диапазоне от 0 до 1, вы можете контролировать баланс между созданием локально согласованного и разнообразного текста. Значение, близкое к 1, будет способствовать более контекстуально согласованным токенам, в то время как значение, близкое к 0, будет способствовать более разнообразным токенам. Значение, равное 1, отключает локально типичную выборку.

Пример использования: `--typical 0.9`

### Выборка Mirostat

- `--mirostat N`: Включить выборку Mirostat, контролируя путаницу при генерации текста (по умолчанию: 0, 0 = отключено, 1 = Mirostat, 2 = Mirostat 2.0).
- `--mirostat-lr N`: Установите скорость обучения Mirostat, параметр eta (по умолчанию: 0.1).
- `--mirostat-ent N`: Установите целевую энтропию Mirostat, параметр tau (по умолчанию: 5.0).

Mirostat - это алгоритм, который активно поддерживает качество сгенерированного текста в желаемом диапазоне во время генерации текста. Он направлен на достижение баланса между согласованностью и разнообразием, избегая некачественного вывода, вызванного чрезмерным повторением (ловушки скуки) или непоследовательностью (ловушки путаницы).

Параметр `--mirostat-lr` устанавливает скорость обучения Mirostat (eta). Скорость обучения влияет на то, насколько быстро алгоритм реагирует на обратную связь от сгенерированного текста. Более низкая скорость обучения приведет к более медленной корректировке, в то время как более высокая скорость обучения сделает алгоритм более отзывчивым. Значение по умолчанию - `0.1`.

Параметр `--mirostat-ent` устанавливает целевую энтропию Mirostat (tau), которая представляет желаемое значение запутанности для сгенерированного текста. Настройка целевой энтропии позволяет контролировать баланс между связностью и разнообразием в сгенерированном тексте. Меньшее значение приведет к получению более сфокусированного и связного текста, в то время как более высокое значение приведет к получению более разнообразного и потенциально менее связного текста. Значение по умолчанию - `5.0`.

Пример использования: `--mirostat 2 --mirostat-lr 0.05 --mirostat-ent 3.0`

### Логит-смещение

- `-l TOKEN_ID(+/-)СМЕЩЕНИЕ, --logit-смещение TOKEN_ID(+/-)СМЕЩЕНИЕ`: Измените вероятность появления токена в сгенерированном текстовом завершении.

Параметр logit bias позволяет вам вручную настроить вероятность появления определенных токенов в сгенерированном тексте. Указав идентификатор токена и положительное или отрицательное значение смещения, вы можете увеличить или уменьшить вероятность создания этого токена.

Например, используйте `--logit-bias 15043+1`, чтобы увеличить вероятность появления токена 'Hello', или `--logit-bias 15043-1`, чтобы уменьшить его вероятность. Используя значение отрицательной бесконечности, `--logit-bias 15043-inf` гарантирует, что токен `Hello` никогда не будет создан.

Более практичным вариантом использования могло бы быть предотвращение генерации `\code{begin}` и `\code{end}`, установив токен `\` (29905) на отрицательную бесконечность с помощью `-l 29905-inf`. (Это связано с распространенностью кодов LaTeX, которые отображаются при выводе модели LLaMA.)

Пример использования: `--logit-bias 29905-inf`

### Начальное значение RNG

- `-s SEED, --seed SEED`: Установите начальное значение генератора случайных чисел (RNG) (по умолчанию: -1, -1 = случайное начальное значение).

Начальное значение RNG используется для инициализации генератора случайных чисел, который влияет на процесс генерации текста. Установив определенное начальное значение, вы можете получать согласованные и воспроизводимые результаты при нескольких запусках с одинаковыми входными данными и настройками. Это может быть полезно для тестирования, отладки или сравнения влияния различных параметров на сгенерированный текст, чтобы увидеть, когда они расходятся. Если для начального значения задано значение меньше 0, будет использоваться случайное начальное значение, что приведет к различным результатам при каждом запуске.

## Настройка производительности и параметров памяти

Эти параметры помогают повысить производительность и использование памяти моделей LLaMA. Регулируя эти параметры, вы можете точно настроить поведение модели в соответствии с возможностями вашей системы и достичь оптимальной производительности для вашего конкретного случая использования.

### Количество потоков

- `-t N, --threads N`: Установите количество потоков, которые будут использоваться во время генерации. Для оптимальной производительности рекомендуется установить это значение равным количеству физических ядер процессора, имеющихся в вашей системе (в отличие от логического количества ядер). Использование правильного количества потоков может значительно повысить производительность.
- `-tb N, --threads-batch N`: Установите количество потоков, которые будут использоваться во время пакетной и оперативной обработки. В некоторых системах выгодно использовать большее количество потоков во время пакетной обработки, чем во время генерации. Если не указано, количество потоков, используемых для пакетной обработки, будет таким же, как количество потоков, используемых для генерации.

### Mlock

- `--mlock`: Блокирует модель в памяти, предотвращая ее выгрузку при сопоставлении с памятью. Это может повысить производительность, но сводит на нет некоторые преимущества сопоставления с памятью, поскольку для запуска требуется больше оперативной памяти и потенциально замедляется время загрузки модели в ОЗУ.

### Нет сопоставления с памятью

- `--no-mmap`: Не отображать модель в память. По умолчанию модели отображаются в память, что позволяет системе загружать только необходимые части модели по мере необходимости. Однако, если размер модели превышает общий объем оперативной памяти или если в вашей системе недостаточно доступной памяти, использование mmap может увеличить риск выпадения страниц, что негативно скажется на производительности. Отключение mmap приводит к замедлению загрузки, но может уменьшить количество просмотров страниц, если вы не используете `--mlock`. Обратите внимание, что если размер модели превышает общий объем оперативной памяти, отключение mmap вообще предотвратит загрузку модели.

### Поддержка NUMA

- `--распределение numa`: Прикрепите равную долю потоков к ядрам на каждом узле NUMA. Это распределит нагрузку между всеми ядрами системы, задействуя все каналы памяти за счет потенциальной необходимости передачи памяти по медленным каналам связи между узлами.
- `--numa isolate`: Привязать все потоки к узлу NUMA, на котором запускается программа. Это ограничивает количество ядер и объем памяти, которые могут быть использованы, но гарантирует, что весь доступ к памяти остается локальным для узла NUMA.
- `--numa numactl`: Привязать потоки к CPUMAP, который передается программе, запустив ее с помощью утилиты numactl. Это наиболее гибкий режим, позволяющий использовать произвольные шаблоны использования ядер, например, карту, которая использует все ядра на одном узле NUMA и достаточное количество ядер на втором узле для насыщения межузловой шины памяти.

 Эти флаги позволяют выполнить оптимизацию, которая помогает в некоторых системах с неравномерным доступом к памяти. В настоящее время это включает в себя одну из вышеуказанных стратегий и отключение предварительной выборки и readahead для mmap. Последнее приводит к сбою сопоставленных страниц при первом доступе, а не ко всем сразу, и в сочетании с привязкой потоков к узлам NUMA большее количество страниц оказывается на узле NUMA, где они используются. Обратите внимание, что если модель уже находится в системном кэше страниц, например, из-за предыдущего запуска без этой опции, это будет иметь незначительный эффект, если вы сначала не удалите кэш страниц. Это можно сделать, перезагрузив систему или в Linux, записав '3' в '/proc/sys/vm/drop_caches' от имени root.

### Объем памяти 32

- `--memory-f32`: Используйте 32-разрядные значения с плавающей запятой вместо 16-разрядных значений с плавающей запятой для ключа памяти+значения. Это удваивает требования к контекстной памяти и размер кэшированного файла запроса, но, по-видимому, не повышает качество генерации измеримым образом. Не рекомендуется.

### Размер партии

- `-b N, --batch-size N`: Установите размер пакета для оперативной обработки (по умолчанию: 512). Этот большой размер пакета выгоден пользователям, у которых BLAS установлен и включен во время сборки. Если у вас не включен BLAS ("BLAS=0"), вы можете использовать меньшее число, например 8, чтобы увидеть ход выполнения запроса по мере его оценки в некоторых ситуациях.

